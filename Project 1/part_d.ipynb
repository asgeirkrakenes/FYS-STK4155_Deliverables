{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c8c59f8",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html Project1.do.txt  -->\n",
    "<!-- dom:TITLE: Project 1 on Machine Learning, deadline October 7 (midnight), 2024 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d1dd3",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Part d): Paper and pencil part\n",
    "\n",
    "This exercise deals with various mean values and variances in  linear regression method (here it may be useful to look up chapter 3, equation (3.8) of [Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, The Elements of Statistical Learning, Springer](https://www.springer.com/gp/book/9780387848570)). The exercise is also part of the weekly exercises for week 37.\n",
    "\n",
    "The assumption we have made is \n",
    "that there exists a continuous function $f(\\boldsymbol{x})$ and  a normal distributed error $\\boldsymbol{\\varepsilon}\\sim N(0, \\sigma^2)$\n",
    "which describes our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fab7b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{y} = f(\\boldsymbol{x})+\\boldsymbol{\\varepsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b7fb9",
   "metadata": {
    "editable": true
   },
   "source": [
    "We then approximate this function $f(\\boldsymbol{x})$ with our model $\\boldsymbol{\\tilde{y}}$ from the solution of the linear regression equations (ordinary least squares OLS), that is our\n",
    "function $f$ is approximated by $\\boldsymbol{\\tilde{y}}$ where we minimized  $(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2$, with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf0a0d5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\tilde{y}} = \\boldsymbol{X}\\boldsymbol{\\beta}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c48acb",
   "metadata": {
    "editable": true
   },
   "source": [
    "The matrix $\\boldsymbol{X}$ is the so-called design or feature matrix. \n",
    "\n",
    "Show that  the expectation value of $\\boldsymbol{y}$ for a given element $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158357a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbb{E}(y_i)  =\\sum_{j}x_{ij} \\beta_j=\\mathbf{X}_{i, \\ast} \\, \\boldsymbol{\\beta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5d694",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "Using the linearity of expectation:\n",
    "\n",
    "$\\mathbb{E}(y) = \\mathbb{E}(f(x) + \\epsilon) = \\mathbb{E}(\\hat{y} + \\epsilon) = \\mathbb{E}(\\hat{y}) + \\mathbb{E}(\\epsilon) = \\mathbb{E}(\\hat{y}) = \\mathbb{E}(\\mathbf{X}\\boldsymbol{\\beta})$\n",
    "\n",
    "The expected value of the residual $\\epsilon$ is zero because we assume it is normally distributed with a mean of zero: $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "Therefore, for any $y_i$, we have:\n",
    "$\\mathbb{E}(y_i) = \\mathbb{E}\\left(\\sum_j X_{i,j}\\beta_j\\right) = \\sum_j \\mathbb{E}(X_{i,j}\\beta_j) = \\mathbf{X}_{i,*}\\boldsymbol{\\beta}$, where $*$ denotes some value $i$.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021253bc",
   "metadata": {
    "editable": true
   },
   "source": [
    "and that\n",
    "its variance is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89d5fe",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\text{Var}(y_i)  = \\sigma^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f37232",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "$$\n",
    "[ \\text{Var}(y) = \\text{Var}(f(\\boldsymbol{x}) + \\varepsilon) ]\n",
    "$$\n",
    "\n",
    "Since $f(\\boldsymbol{x})$ is deterministic and does not vary:\n",
    "\n",
    "$\\text{Var}(f(\\boldsymbol{x})) = 0$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Var}(y) = \\text{Var}(f(\\boldsymbol{x})) + \\text{Var}(\\varepsilon) + 2 \\text{Cov}(f(\\boldsymbol{x}), \\varepsilon)\n",
    "$$\n",
    "\n",
    "Given that $f(\\boldsymbol{x})$ is deterministic and independent of the error term $\\varepsilon$, as we assume through OLS that there are no correlations between the independent variables and the error term, the covariance term is zero:\n",
    "\n",
    "$\\text{Cov}(f(\\boldsymbol{x}), \\varepsilon) = 0$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$\\text{Var}(y) = \\text{Var}(\\varepsilon) = \\sigma^2$\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb7a84",
   "metadata": {},
   "source": [
    "\n",
    "Hence, $y_i \\sim N( \\mathbf{X}_{i, \\ast} \\, \\boldsymbol{\\beta}, \\sigma^2)$, that is $\\boldsymbol{y}$ follows a normal distribution with \n",
    "mean value $\\boldsymbol{X}\\boldsymbol{\\beta}$ and variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710aec68",
   "metadata": {},
   "source": [
    "With the OLS expressions for the optimal parameters $\\boldsymbol{\\hat{\\beta}}$ show that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e62eff",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}}) = \\boldsymbol{\\beta}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a8e093",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "We have that:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\beta}} = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "and that\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} = f(\\boldsymbol{x}) + \\boldsymbol{\\varepsilon} = \\boldsymbol{\\tilde{y}} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}.\n",
    "$$\n",
    "\n",
    "From here:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}}) = \\mathbb{E}((X^T X)^{-1} X^T y) = \\mathbb{E}((X^T X)^{-1} X^T (\\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}))\n",
    "$$\n",
    "\n",
    "Using the linearity of expectation, we get:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}}) &= \\mathbb{E}((X^T X)^{-1} (X^T \\boldsymbol{X} \\boldsymbol{\\beta} + X^T \\boldsymbol{\\varepsilon})) \\\\\n",
    "&= \\mathbb{E}((X^T X)^{-1} X^T \\boldsymbol{X} \\boldsymbol{\\beta}) + \\mathbb{E}((X^T X)^{-1} X^T \\boldsymbol{\\varepsilon})\n",
    "\\end{align*}\n",
    "\n",
    "From earlier, we have that:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}((X^T X)^{-1} X^T \\boldsymbol{X} \\boldsymbol{\\beta}) = \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "This is because the term $((X^T X)^{-1} X^T X)$ simplifies to the identity matrix $I$, and that the expectation of $\\boldsymbol{\\beta}$ is simply $\\boldsymbol{\\beta}$, as the expectation of a constant evaluates to.\n",
    "\n",
    "We then have that:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}((X^T X)^{-1} (X^T \\boldsymbol{X} \\boldsymbol{\\beta} + X^T \\boldsymbol{\\varepsilon})) = \\boldsymbol{\\beta} + \\mathbb{E}((X^T X)^{-1} X^T \\boldsymbol{\\varepsilon})\n",
    "$$\n",
    "\n",
    "Using the fact that $(\\mathbb{E}(\\boldsymbol{\\varepsilon}) = 0)$ and that $((X^T X)^{-1} X^T)$ is deterministic and can then be put outside of the expectation, we get:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}((X^T X)^{-1} X^T \\boldsymbol{\\varepsilon}) = (X^T X)^{-1} X^T \\mathbb{E}(\\boldsymbol{\\varepsilon}) = (X^T X)^{-1} X^T \\cdot 0 = 0\n",
    "$$\n",
    "\n",
    "Therefore, combining these results, we obtain:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}}) = \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "Hence, $\\boldsymbol{\\hat{\\beta}}$ is an unbiased estimator of $\\boldsymbol{\\beta}$.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e833f14",
   "metadata": {
    "editable": true
   },
   "source": [
    "Show finally that the variance of $\\boldsymbol{\\beta}$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef5a97",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\text{Var}(\\boldsymbol{\\hat{\\beta}}) = \\sigma^2 \\, (\\mathbf{X}^{T} \\mathbf{X})^{-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba1135",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "We have that:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1} X^T y = (X^T X)^{-1} X^T X \\hat{\\beta} + (X^T X)^{-1} X^T \\epsilon = I \\hat{\\beta} + (X^T X)^{-1} X^T \\epsilon = \\hat{\\beta} + (X^T X)^{-1} X^T \\epsilon\n",
    "$$\n",
    "\n",
    "From here, while noting that the $\\hat{\\beta}$ column vector is deterministic and thus does not vary:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta} + (X^T X)^{-1} X^T \\epsilon) = \\text{Var}((X^T X)^{-1} X^T \\epsilon)\n",
    "$$\n",
    "\n",
    "Further, for one constant matrix $A$ and a random column vector $\\mathbf{X}$, we have that:\n",
    "\n",
    "$$\n",
    "\\text{Var}(A \\mathbf{X}) = \\mathbb{E}\\left[A (\\mathbf{X} - \\boldsymbol{\\mu})(A (\\mathbf{X} - \\boldsymbol{\\mu}))^T \\right] = \\mathbb{E}\\left[ A (\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T A^T \\right] = A \\mathbb{E}\\left[ (\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T \\right] A^T = A \\text{Var}(\\mathbf{X}) A^T\n",
    "$$\n",
    "\n",
    "Thus, knowing that $\\text{Var}(\\epsilon) = \\sigma^2$ and that $((X^T X)^{-1}X^T)^{T} = (X(X^T X)^{-1})$ :\n",
    "\n",
    "$$\n",
    "\\text{Var}((X^T X)^{-1} X^T \\epsilon) = (X^T X)^{-1} X^T \\text{Var}(\\epsilon) X (X^T X)^{-1} = (X^T X)^{-1} X^T \\sigma^2 I X (X^T X)^{-1}\n",
    "$$\n",
    "\n",
    "Factoring out $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "= \\sigma^2 (X^T X)^{-1} X^T X (X^T X)^{-1} = \\sigma^2 (X^T X)^{-1}\n",
    "$$\n",
    "\n",
    "Thus, the final simplified variance expression is:\n",
    "\n",
    "$$\n",
    "\\text{Var}((X^T X)^{-1} X^T \\epsilon) = \\sigma^2 (X^T X)^{-1}\n",
    "$$\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9443b1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can use the last expression when we define a so-called confidence interval for the parameters $\\beta$. \n",
    "A given parameter $\\beta_j$ is given by the diagonal matrix element of the above matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
